Samsung Codebook
README file

Samsung Codebook

Samsung has generated some data using personal wearable devices the details of which are given in
the Samsung code book, incorporated herein by reference. The Samsung descriptive documents should 
be read prior to this file if further explanation is needed. The data generated by Samsun was 
divided into two parts 1) test and 2) train. The sensor data was further refined using 
hardware/software techniques and a resulting normalized (and therefore, unitless) dataset was 
provided to us.

We were given: Subject, activity and measurement data for both test and training datasets. Our 
task was 1) to merge the subject, activity and measurement data for test data set 2) do the same 
for the training dataset 3) then merge the test and training data set to yield one merged data set
4) extract mean measurements from the merged dataset by activity snd subject. 5) provide this as a 
tidy data set.

The code written takes as input the data and codebooks supplied from the Samsung wearables 
project provided as a zip file. Data and documenation supplied contains details of 
how data was gathered and computed and are incorporated herein by reference. Those
specifics will not be repeated in this documentation. This documentation will only
describe the post processing/computing done by me via the R script that is provided


1. This code reads the test data as well as the training data measurements (please 
see the codebooks that come with the supplied data for details about how this data was gathered),
as well as the non-measurement data pertaining to the Activity and Subject. 

2. First, The test Activity and Subject data is merged with the test data. Then the training 
Activity and Subject data is merged with and training data. 

3. Then the test and training datasets (fron 2 above) are merged together to 
yield one data set. In the merge the TRAINING dataset is placed ahead of TEST data
TRAINING data occupies the first 7352 rows and the 2947 rows of the TEST data starts from row 7353 
onwards through row 10299. 

4. The test and training data have 561 variables/columns, Activity and 
Subject have one column each, so the total merged dataset (in 3 above) has 563 columns.

5. From these 563 columns I have extracted the data pertaining to the mean and standard 
deviation only as reuired per course rubric. There were 53 columns for mean and 33 columns
for standard deviation in the measurement data of 561 columns. So from the dataset yielded 
(in Step 3), 86 columns of measurement data as well as one column of Subject and activity 
data were extracted for a total 88 columns. 

6. To figure which 86 of the 561 columns had to be extracted, I loaded the features.txt 
file which contained the column names into Excel and looked up all the columns that had 
"mean"(mean) and standard deviation (std) in the column names. I also used global find and 
replace in Excel to clean up the column names by removing all commas, dashes
parenthesis etc. from the names and gave them some English names and made them conform to 
Pascal case notation to make it easier to read the column names and know what they signify. 
Then I saved the column names and numbers from Excel as a text file and imported that file
in R which gave me the column numbers to be extracted and their cleaned up names from Excel.
into R by cutting and pasting. Once this was done, I did the relevant column extraction in R.
A list of columns numbers extracted in included later in this file under "INDEX OF EXTRACTED
COLUMNS"

7. From the dataset created in step 6, I calculated the mean of the column measurements by 
activity and subject. There are 6 activity types and 30 subjects in the data. This makes 
for 6x30 rows that are within the dataset. Each element in the row represents the mean for 
that measurement in the data in step 6, broken by activity and subject. There are 86 
measurement columns in Step 6 for which the mean was calculatd. So there are 180 rows and 
86 calculated columns along with one columns denoting the subject and activity type for 
the row for a total of 180 rows and 87 columns.

8. The data created in step 3 was written out to the output file as the final result. A 
description of the columns in the output
tidy data set is included in the file "Columns.txt" which is contained in this repo.

INDEX OF EXTRACTED COLUMNS
1,
2,
3,
4,
5,
6,
41,
42,
43,
44,
45,
46,
81,
82,
83,
84,
85,
86,
121,
122,
123,
124,
125,
126,
161,
162,
163,
164,
165,
166,
201,
202,
214,
215,
227,
228,
240,
241,
253,
254,
266,
267,
268,
269,
270,
271,
294,
295,
296,
345,
346,
347,
348,
349,
350,
373,
374,
375,
424,
425,
426,
427,
428,
429,
452,
453,
454,
503,
504,
513,
516,
517,
526,
529,
530,
539,
542,
543,
552,
555,
556,
557,
558,
559,
560,
561,
562,
563




